{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual LSTM encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Retrieval-based_Chatbot](https://github.com/Janinanu/Retrieval-based_Chatbot).\n",
    "\n",
    "Find the necessary data files within this [Google Drive folder](https://drive.google.com/open?id=1RIIbsS-vxR7Dlo2_v6FWHDFE7q1XPPgj).\n",
    "\n",
    "Other useful read: paper of [Ubuntu Dialogue Corpus](https://arxiv.org/pdf/1506.08909.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import init\n",
    "import torch.nn.utils.rnn \n",
    "import datetime\n",
    "import operator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAINING_SET = 'data/train_both_original.txt'\n",
    "PATH_TO_VALIDATION_SET = 'data/valid_both_original.txt'\n",
    "PATH_TO_WEIGHTS = 'data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize the given sentence by:\n",
    "        - Converting to lower case\n",
    "        - Transforming the English contractions to full words\n",
    "        - Transforming composed words into 2 separated words\n",
    "        - Tokenizing into words based on white space\n",
    "    Inspired from : https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "    and https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    sentence = sentence.replace(\"'s\", ' is')\n",
    "    sentence = sentence.replace(\"n't\", ' not')\n",
    "    sentence = sentence.replace(\"'re\", ' are')\n",
    "    sentence = sentence.replace(\"'m\", ' am')\n",
    "    sentence = sentence.replace(\"'ve\", ' have')\n",
    "    sentence = sentence.replace(\"'ll\", ' will')\n",
    "    sentence = sentence.replace(\"'d\", ' would')\n",
    "    sentence = sentence.replace(\"-\", ' ')\n",
    "    sentence = sentence.replace(\"!\", ' ! ')\n",
    "    sentence = sentence.replace(\".\", ' . ')\n",
    "    sentence = sentence.replace(\":\", ' : ')\n",
    "\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset_as_text(path: str, is_training_set: bool, nb_dialogues=-1) -> tuple:\n",
    "    \"\"\"\n",
    "    Extract the dataset as text lists. If it is a training dataset, the first answer will be the correct one, others are\n",
    "    distractors.\n",
    "    To access an element of a dialogue i in one of the following list (e.g: output_list), do: output_list[i].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to training file.\n",
    "    is_training_set : bool\n",
    "        Whether the dataset to extract is a training set (correct answer is known).\n",
    "    nb_dialogues : int\n",
    "        Number of dialogues to extract. Set -1 for all.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : tuple\n",
    "        word_to_id : Dict[str, int]\n",
    "            Vocabulary mapping each word to an unique id. Only returned if it's a training set.\n",
    "        my_personae : List[List[List[str]]]\n",
    "            My personae of each dialogue.\n",
    "        other_personae : List[List[List[str]]]\n",
    "            Other personae of each dialogue.\n",
    "        line_indices : List[List[int]]\n",
    "            Indices of lines, except those describing the persona, of each dialogue.\n",
    "        utterances : List[List[List[str]]]\n",
    "            Utterances (question-like) of each dialogue.\n",
    "        answers : List[List[List[List[str]]]]\n",
    "            Answers of each utterance of each dialogue. The correct one (for a training set) for a dialogue i and an\n",
    "            utterance j is answers[i][j][0], the others answers[i][j][k] for k>0 are wrong answers.\n",
    "    \"\"\"\n",
    "    def get_tokens_from_sentence(sentence: str):\n",
    "        \"\"\"Normalizes, extracts the tokens from the given sentence and adds them to the vocabulary.\"\"\"\n",
    "        tokens = normalize(sentence)\n",
    "        if is_training_set:\n",
    "            for element in tokens:\n",
    "                if element not in word_to_id:\n",
    "                    word_to_id[element] = len(word_to_id)\n",
    "        return tokens\n",
    "                \n",
    "    word_to_id = {}\n",
    "    my_personae = []\n",
    "    other_personae = []\n",
    "    line_indices = []\n",
    "    utterances = []\n",
    "    answers = []\n",
    "    idx_dialogue = 0\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            idx_line = int(words[0])\n",
    "            if idx_line == 1:\n",
    "                idx_dialogue += 1\n",
    "                if idx_dialogue == nb_dialogues + 1:\n",
    "                    break\n",
    "\n",
    "            # Get my persona\n",
    "            if words[1] + ' ' + words[2] == 'your persona:':\n",
    "                if len(my_personae) != idx_dialogue:\n",
    "                    my_personae.append([])\n",
    "                my_personae[-1].append(get_tokens_from_sentence(' '.join(str(word) for word in words[3:])))\n",
    "\n",
    "            # Get other persona\n",
    "            elif words[1] + ' ' + words[2] == \"partner's persona:\":\n",
    "                if len(other_personae) != idx_dialogue:\n",
    "                    other_personae.append([])\n",
    "                other_personae[-1].append(get_tokens_from_sentence(' '.join(str(word) for word in words[3:])))\n",
    "\n",
    "            # Get dialogue\n",
    "            else:\n",
    "                if len(utterances) != idx_dialogue:\n",
    "                    line_indices.append([])\n",
    "                    utterances.append([])\n",
    "                    answers.append([])\n",
    "\n",
    "                line_indices[-1].append(idx_line)\n",
    "                exchange = line[len(str(idx_line)) + 1:].split('\\t')\n",
    "                utterances[-1].append(get_tokens_from_sentence(exchange[0]))\n",
    "                # Training set: answer is known\n",
    "                if is_training_set:\n",
    "                    answers[-1].append([get_tokens_from_sentence(exchange[1])])  # Correct answers\n",
    "                    for statement in exchange[2:]:  # Wrong answers\n",
    "                        if statement == '':\n",
    "                            continue\n",
    "                        for distractor in statement.split('|'):\n",
    "                            answers[-1][-1].append(get_tokens_from_sentence(distractor))\n",
    "                            \n",
    "                # Testing set: answer is unknown\n",
    "                else:\n",
    "                    answers[-1].append([])\n",
    "                    for statement in exchange[1:]:\n",
    "                        if statement == '':\n",
    "                            continue\n",
    "                        for distractor in statement.split('|'):\n",
    "                            answers[-1][-1].append(get_tokens_from_sentence(distractor))\n",
    "                            \n",
    "    print('Loaded ' + str(len(line_indices)) + ' dialogues')\n",
    "    if is_training_set:\n",
    "        return word_to_id, my_personae, other_personae, line_indices, utterances, answers\n",
    "    else:\n",
    "        return my_personae, other_personae, line_indices, utterances, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_to_vec(word_to_id: dict, path_to_glove_weights: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the embedding weights for each word in the vocabulary and maps each word ids to its weight in a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_glove_weights : str\n",
    "        Path to the file containing the embedding weights.\n",
    "    word_to_id : Dict[str, int]\n",
    "        Vocabulary mapping each word to an unique id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    id_to_vec : Dict[int, np.ndarray]\n",
    "        Map of each word id to its embedding form.\n",
    "    \"\"\"\n",
    "    with open(path_to_glove_weights, 'r', encoding='utf-8') as glovefile:\n",
    "        lines = glovefile.readlines()\n",
    "\n",
    "    id_to_vec = {}\n",
    "    vector = None\n",
    "    \n",
    "    for line in lines:\n",
    "        word = line.split()[0]\n",
    "        vector = np.array(line.split()[1:], dtype='float32')\n",
    "        \n",
    "        if word in word_to_id:\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(vector))\n",
    "            \n",
    "    for word, id in word_to_id.items(): \n",
    "        if word_to_id[word] not in id_to_vec:\n",
    "            v = np.zeros(*vector.shape, dtype='float32')\n",
    "            v[:] = np.random.randn(*v.shape)*0.01\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(v))\n",
    "                \n",
    "    return id_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"LSTM encoder\"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, hidden_size, p_dropout, id_to_vec): \n",
    "    \n",
    "            super(Encoder, self).__init__()\n",
    "             \n",
    "            self.emb_size = emb_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.vocab_size = len(id_to_vec)\n",
    "            self.p_dropout = p_dropout\n",
    "       \n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "            self.lstm = nn.LSTM(self.emb_size, self.hidden_size)\n",
    "            self.dropout_layer = nn.Dropout(self.p_dropout) \n",
    "\n",
    "            self.init_weights(id_to_vec)\n",
    "             \n",
    "    def init_weights(self, id_to_vec):\n",
    "        init.uniform_(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n",
    "        init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "        self.lstm.weight_ih_l0.requires_grad = True\n",
    "        self.lstm.weight_hh_l0.requires_grad = True\n",
    "        \n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.emb_size)\n",
    "            \n",
    "        for idx, vec in id_to_vec.items():\n",
    "            embedding_weights[idx] = vec\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(embedding_weights, requires_grad = True)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        _, (last_hidden, _) = self.lstm(embeddings) #dimensions: (num_layers * num_directions x batch_size x hidden_size)\n",
    "        last_hidden = self.dropout_layer(last_hidden[-1])#access last lstm layer, dimensions: (batch_size x hidden_size)\n",
    "\n",
    "        return last_hidden\n",
    "\n",
    "    \n",
    "class DualEncoder(nn.Module):\n",
    "    \"\"\"Dual LSTM encoder\"\"\"\n",
    "     \n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = self.encoder.hidden_size\n",
    "        M = torch.FloatTensor(self.hidden_size, self.hidden_size)     \n",
    "        init.xavier_normal_(M)\n",
    "        self.M = nn.Parameter(M, requires_grad = True)\n",
    "\n",
    "    def forward(self, context_tensor, response_tensor):\n",
    "        \n",
    "        context_last_hidden = self.encoder(context_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        response_last_hidden = self.encoder(response_tensor) #dimensions: (batch_size x hidden_size)\n",
    "        \n",
    "        #context = context_last_hidden.mm(self.M).cuda()\n",
    "        context = context_last_hidden.mm(self.M) #dimensions: (batch_size x hidden_size)\n",
    "        context = context.view(-1, 1, self.hidden_size) #dimensions: (batch_size x 1 x hidden_size)\n",
    "        \n",
    "        response = response_last_hidden.view(-1, self.hidden_size, 1) #dimensions: (batch_size x hidden_size x 1)\n",
    "        \n",
    "        #score = torch.bmm(context, response).view(-1, 1).cuda()\n",
    "        score = torch.bmm(context, response).view(-1, 1) #dimensions: (batch_size x 1 x 1) and lastly --> (batch_size x 1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_training_variables(path_to_training_set, path_to_glove_weights, embedding_dim=50, nb_dialogues=-1):\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for training...\")\n",
    "    \n",
    "    word_to_id, my_personae, other_personae, line_indices, utterances, answers = extract_dataset_as_text(path_to_training_set, True, nb_dialogues)\n",
    "    id_to_vec = create_id_to_vec(word_to_id, path_to_glove_weights)\n",
    "    # Unknown words\n",
    "    v = np.zeros(embedding_dim, dtype='float32')\n",
    "    v[:] = np.random.randn(*v.shape)*0.01\n",
    "    id_to_vec[-1] = torch.FloatTensor(torch.from_numpy(v))\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n",
    "    return id_to_vec, word_to_id, my_personae, other_personae, line_indices, utterances, answers\n",
    "\n",
    "def creating_validation_variables(path_to_validation_set, nb_dialogues=-1):\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for validations...\")\n",
    "    \n",
    "    _, my_personae, other_personae, line_indices, utterances, answers = extract_dataset_as_text(path_to_validation_set, True, nb_dialogues)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n",
    "    return my_personae, other_personae, line_indices, utterances, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_model(emb_size, hidden_size, p_dropout, id_to_vec):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Calling model...\")\n",
    "\n",
    "    encoder = Encoder(emb_size, hidden_size, p_dropout, id_to_vec)\n",
    "\n",
    "    dual_encoder = DualEncoder(encoder)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Model created.\\n\")\n",
    "    print(dual_encoder)\n",
    "    \n",
    "    return dual_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_id(word_to_id: dict, token: str) -> int:\n",
    "    \"\"\"Retrieves the ID of the word if known, else returns -1 (ID for unknown words).\"\"\"\n",
    "    try:\n",
    "        id_word = word_to_id[token]\n",
    "    except KeyError:\n",
    "        id_word = -1\n",
    "    return id_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_data_on_disk(word_to_id, my_personae, other_personae, line_indices, utterances, answers, is_training, max_context_len=160, max_size_df=10000):\n",
    "    dataframe_name = 'validation_df'\n",
    "    if is_training:\n",
    "        with open('data/word_to_id' + '.pkl', 'wb') as dict_file:\n",
    "            pickle.dump(word_to_id, dict_file)\n",
    "        dataframe_name = 'training_df'\n",
    "        \n",
    "    dataframe_to_save = pd.DataFrame(columns=['context', 'response', 'label', 'idx_line'])\n",
    "    idx_dataframe = 0\n",
    "    for idx_dialogue in range(len(line_indices)):\n",
    "        if len(dataframe_to_save) >= max_size_df:\n",
    "            dataframe_to_save.to_csv('data/{0}{1}.csv'.format(dataframe_name, idx_dataframe), index=False)\n",
    "            idx_dataframe += 1\n",
    "            \n",
    "        context_ids = []\n",
    "        # Add my persona in context\n",
    "        for sentence in my_personae[idx_dialogue]:\n",
    "            for token in sentence:\n",
    "                context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "        # Add other persona in context\n",
    "        for sentence in other_personae[idx_dialogue]:\n",
    "            for token in sentence:\n",
    "                context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "        # Add utterances, create responses and labels\n",
    "        for idx_utterance in range(len(utterances[idx_dialogue])):\n",
    "            if idx_utterance != 0:\n",
    "                # Add previous correct answer in context\n",
    "                for token in answers[idx_dialogue][idx_utterance - 1][0]:\n",
    "                    context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "            # Add utterances in context\n",
    "            for token in utterances[idx_dialogue][idx_utterance]:\n",
    "                context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "            # Get response and label\n",
    "            for idx_answer in range(len(answers[idx_dialogue][idx_utterance])):\n",
    "                response_ids = []\n",
    "                for token in answers[idx_dialogue][idx_utterance][idx_answer]:\n",
    "                    response_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "\n",
    "                if idx_answer == 0:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                    \n",
    "                if len(context_ids) > max_context_len:\n",
    "                    context_ids = context_ids[:max_context_len]\n",
    "                if len(response_ids) > max_context_len:\n",
    "                    response_ids = response_ids[:max_context_len]\n",
    "\n",
    "                dataframe_to_save.loc[len(dataframe_to_save)] = [0, 0, label, line_indices[idx_dialogue][idx_utterance]]\n",
    "                dataframe_to_save['context'][len(dataframe_to_save) - 1] = context_ids\n",
    "                dataframe_to_save['response'][len(dataframe_to_save) - 1] = response_ids\n",
    "    \n",
    "    dataframe_to_save.to_csv('data/{0}{1}.csv'.format(dataframe_name, idx_dataframe), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, l2_penalty, nb_epochs, dual_encoder, word_to_id,\n",
    "               tr_my_personae, tr_other_personae, tr_line_indices, tr_utterances, tr_answers,\n",
    "               val_my_personae, val_other_personae, val_line_indices, val_utterances, val_answers):\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Starting training...\\n\")\n",
    "#     print(\"====================Data and Hyperparameter Overview====================\\n\")\n",
    "#     print(\"Number of training examples: %d, Number of validation examples: %d\" %(len(training_dataframe), len(validation_dataframe)))\n",
    "#     print(\"Learning rate: %.5f, Embedding Dimension: %d, Hidden Size: %d, Dropout: %.2f, L2:%.10f\\n\" %(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n",
    "#     print(\"================================Results...==============================\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "       \n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "          \n",
    "    for epoch in range(nb_epochs):\n",
    "        # TODO : shuffle data here\n",
    "        sum_loss_training = 0\n",
    "        nb_iter_tr = 0\n",
    "        sum_loss_validation = 0\n",
    "        nb_iter_val = 0\n",
    "\n",
    "        # First: use training set\n",
    "        dual_encoder.train()\n",
    "\n",
    "        for idx_dialogue in range(len(tr_line_indices)):\n",
    "            context_ids = []\n",
    "            # Add my persona in context\n",
    "            for sentence in tr_my_personae[idx_dialogue]:\n",
    "                for token in sentence:\n",
    "                    context_ids.append(word_to_id[token])\n",
    "\n",
    "            # Add other persona in context\n",
    "            for sentence in tr_other_personae[idx_dialogue]:\n",
    "                for token in sentence:\n",
    "                    context_ids.append(word_to_id[token])\n",
    "\n",
    "            # Add utterances, create responses and labels\n",
    "            for idx_utterance in range(len(tr_utterances[idx_dialogue])):\n",
    "                if idx_utterance != 0:\n",
    "                    # Add previous correct answer in context\n",
    "                    for token in tr_answers[idx_dialogue][idx_utterance - 1][0]:\n",
    "                        context_ids.append(word_to_id[token])\n",
    "\n",
    "                # Add utterances in context\n",
    "                for token in tr_utterances[idx_dialogue][idx_utterance]:\n",
    "                    context_ids.append(word_to_id[token])\n",
    "\n",
    "                # Get response and label\n",
    "                for idx_answer in range(len(tr_answers[idx_dialogue][idx_utterance])):\n",
    "                    response_ids = []\n",
    "                    for token in tr_answers[idx_dialogue][idx_utterance][idx_answer]:\n",
    "                        response_ids.append(word_to_id[token])\n",
    "\n",
    "                    if idx_answer == 0:\n",
    "                        label = np.array(1).astype(np.float32)\n",
    "                    else:\n",
    "                        label = np.array(0).astype(np.float32)\n",
    "\n",
    "                    context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False).cuda()\n",
    "                    response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) .cuda()   \n",
    "                    label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False).cuda()\n",
    "\n",
    "                    # Predict\n",
    "                    score = dual_encoder(context, response)\n",
    "                    loss = loss_func(score, label)\n",
    "                    \n",
    "                    # Train\n",
    "                    nb_iter_tr += 1\n",
    "                    sum_loss_training += loss.data.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "        # Second: use validation set\n",
    "        # TODO : shuffle data here\n",
    "#         for idx_dialogue in range(len(val_line_indices)):\n",
    "#             context_ids = []\n",
    "#             # Add my persona in context\n",
    "#             for sentence in val_my_personae[idx_dialogue]:\n",
    "#                 for token in sentence:\n",
    "#                     context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "#             # Add other persona in context\n",
    "#             for sentence in val_other_personae[idx_dialogue]:\n",
    "#                 for token in sentence:\n",
    "#                     context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "#             # Add utterances, create responses and labels\n",
    "#             for idx_utterance in range(len(val_utterances[idx_dialogue])):\n",
    "#                 if idx_utterance != 0:\n",
    "#                     # Add previous correct answer in context\n",
    "#                     for token in val_answers[idx_dialogue][idx_utterance - 1][0]:\n",
    "#                         context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "#                 # Add utterances in context\n",
    "#                 for token in val_utterances[idx_dialogue][idx_utterance]:\n",
    "#                     context_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "#                 # Get response and label\n",
    "#                 for idx_answer in range(len(val_answers[idx_dialogue][idx_utterance])):\n",
    "#                     response_ids = []\n",
    "#                     for token in val_answers[idx_dialogue][idx_utterance][idx_answer]:\n",
    "#                         response_ids.append(get_word_id(word_to_id, token))\n",
    "\n",
    "#                     if idx_answer == 0:\n",
    "#                         label = np.array(1).astype(np.float32)\n",
    "#                     else:\n",
    "#                         label = np.array(0).astype(np.float32)\n",
    "\n",
    "#                     context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False).cuda()\n",
    "#                     response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) .cuda()   \n",
    "#                     label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False).cuda()\n",
    "\n",
    "#                     # Predict\n",
    "#                     score = dual_encoder(context, response)\n",
    "#                     loss = loss_func(score, label)\n",
    "#                     nb_iter_val += 1\n",
    "#                     sum_loss_validation += loss.data.item()\n",
    "        \n",
    "        \n",
    "        print('Training loss =', sum_loss_training / nb_iter_tr)\n",
    "#         print('Validation loss =', sum_loss_validation / nb_iter_val)\n",
    "                \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")\n",
    "    return dual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_df(learning_rate, l2_penalty, nb_epochs, dual_encoder, word_to_id):\n",
    "    \"\"\"Training with dataframe\"\"\"\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Starting training...\\n\")\n",
    "#     print(\"====================Data and Hyperparameter Overview====================\\n\")\n",
    "#     print(\"Number of training examples: %d, Number of validation examples: %d\" %(len(training_dataframe), len(validation_dataframe)))\n",
    "#     print(\"Learning rate: %.5f, Embedding Dimension: %d, Hidden Size: %d, Dropout: %.2f, L2:%.10f\\n\" %(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n",
    "#     print(\"================================Results...==============================\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "          \n",
    "    for epoch in range(nb_epochs):\n",
    "        sum_loss_training = 0\n",
    "        nb_iter_tr = 0\n",
    "        sum_loss_validation = 0\n",
    "        nb_iter_val = 0\n",
    "\n",
    "        # First: use training set\n",
    "        dual_encoder.train()\n",
    "        for training_df_name in glob.glob('data/training_df*'):\n",
    "            training_df = pd.read_csv(training_df_name).sample(frac=1)  # Shuffle\n",
    "            \n",
    "            for idx, row in training_df.iterrows():\n",
    "            \n",
    "                context_ids = list(map(int, row['context'][1:-1].split(', ')))\n",
    "                response_ids = list(map(int, row['response'][1:-1].split(', ')))\n",
    "                label = np.array(row['label']).astype(np.float32)\n",
    "\n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False).cuda()\n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) .cuda()   \n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1, 1))), requires_grad = False).cuda()\n",
    "\n",
    "                # Predict\n",
    "                score = dual_encoder(context, response)\n",
    "                loss = loss_func(score, label)\n",
    "\n",
    "                # Train\n",
    "                nb_iter_tr += 1\n",
    "                sum_loss_training += loss.data.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # Second: use validation set\n",
    "#         dual_encoder.eval()\n",
    "#         for validation_df_name in glob.glob('data/validation_df*'):\n",
    "#             validation_df = pd.read_csv(validation_df_name).sample(frac=1)  # Shuffle\n",
    "\n",
    "#             for idx, row in validation_df.iterrows():\n",
    "\n",
    "#                 context_ids = list(map(int, row['context'][1:-1].split(', ')))\n",
    "#                 response_ids = list(map(int, row['response'][1:-1].split(', ')))\n",
    "#                 label = np.array(row['label']).astype(np.float32)\n",
    "\n",
    "#                 context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False).cuda()\n",
    "#                 response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) .cuda()   \n",
    "#                 label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1, 1))), requires_grad = False).cuda()\n",
    "\n",
    "#                 # Predict\n",
    "#                 score = dual_encoder(context, response)\n",
    "#                 loss = loss_func(score, label)\n",
    "#                 nb_iter_val += 1\n",
    "#                 sum_loss_validation += loss.data.item()\n",
    "        \n",
    "        \n",
    "        print('Training loss =', sum_loss_training / nb_iter_tr)\n",
    "#         print('Validation loss =', sum_loss_validation / nb_iter_val)\n",
    "                \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")\n",
    "    return dual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "NB_DIALOGUES = 2  # Set -1 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 02:32:25 Creating variables for training...\n",
      "Loaded 2 dialogues\n",
      "2019-04-15 02:32:34 Variables created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_to_vec, word_to_id, tr_my_personae, tr_other_personae, tr_line_indices, tr_utterances, tr_answers = creating_training_variables(PATH_TO_TRAINING_SET, PATH_TO_WEIGHTS, EMBEDDING_DIM, NB_DIALOGUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 02:32:34 Creating variables for validations...\n",
      "Loaded 2 dialogues\n",
      "2019-04-15 02:32:34 Variables created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_my_personae, val_other_personae, val_line_indices, val_utterances, val_answers = creating_validation_variables(PATH_TO_VALIDATION_SET, NB_DIALOGUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_training_data_on_disk(word_to_id, tr_my_personae, tr_other_personae, tr_line_indices, tr_utterances, tr_answers, True)\n",
    "save_training_data_on_disk(word_to_id, val_my_personae, val_other_personae, val_line_indices, val_utterances, val_answers, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 02:32:35 Calling model...\n",
      "2019-04-15 02:32:35 Model created.\n",
      "\n",
      "DualEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(854, 50)\n",
      "    (lstm): LSTM(50, 50)\n",
      "    (dropout_layer): Dropout(p=0.1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dual_encoder = creating_model(EMBEDDING_DIM, 50, 0.1, id_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 02:33:43 Starting training...\n",
      "\n",
      "Training loss = 0.18769825651266037\n",
      "Training loss = 0.18823424732268212\n",
      "Training loss = 0.18532153655660852\n",
      "Training loss = 0.1777685421097669\n",
      "Training loss = 0.17361444199685844\n",
      "Training loss = 0.17935311002973595\n",
      "Training loss = 0.17824914578988285\n",
      "Training loss = 0.17020619477835947\n",
      "Training loss = 0.16038986673851047\n",
      "Training loss = 0.15296309048683718\n",
      "2019-04-15 02:34:31 Training and validation epochs finished.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "l2_penalty = 1e-4\n",
    "nb_epochs = 10\n",
    "dual_encoder = train_model(learning_rate, l2_penalty, nb_epochs, dual_encoder, word_to_id,\n",
    "                           tr_my_personae, tr_other_personae, tr_line_indices, tr_utterances, tr_answers,\n",
    "                           val_my_personae, val_other_personae, val_line_indices, val_utterances, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-15 02:32:41 Starting training...\n",
      "\n",
      "Training loss = 0.31649383706861134\n",
      "Training loss = 0.20400283946164688\n",
      "Training loss = 0.2031813178220954\n",
      "Training loss = 0.1997427689529271\n",
      "Training loss = 0.19331151172423697\n",
      "Training loss = 0.1914491235424264\n",
      "Training loss = 0.19986246750677372\n",
      "Training loss = 0.19108849731242272\n",
      "Training loss = 0.19430851138222227\n",
      "Training loss = 0.1893954094907991\n",
      "2019-04-15 02:33:31 Training and validation epochs finished.\n"
     ]
    }
   ],
   "source": [
    "# Training with dataframe\n",
    "learning_rate = 1e-4\n",
    "l2_penalty = 1e-4\n",
    "nb_epochs = 10\n",
    "dual_encoder = train_model_df(learning_rate, l2_penalty, nb_epochs, dual_encoder, word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__\n",
    "- Shuffle dataset before each epoch\n",
    "- Plot learning curves\n",
    "- Save model after training\n",
    "- Tests functions\n",
    "- Optimize parameters\n",
    "\n",
    "If it's too slow or kernel dies, try using `.py` scripts (or remove validation part from training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.7_pytorch_RL",
   "language": "python",
   "name": "python_3.7_pytorch_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
