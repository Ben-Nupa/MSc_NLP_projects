{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from NLPex3 import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAINING_SET = 'data/train_both_original.txt'\n",
    "PATH_TO_VALIDATION_SET = 'data/valid_both_original.txt'\n",
    "PATH_TO_WEIGHTS = 'data/crawl-300d-200k.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17878 dialogues\n"
     ]
    }
   ],
   "source": [
    "my_personae, other_personae, line_indices, utterances, answers = extract_dataset_as_text(PATH_TO_TRAINING_SET, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedWord2Vec:\n",
    "    def __init__(self, fname: str, nmax=150000, parser='en'):\n",
    "        self.word2vec = {}\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {w: i for i, w in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "        self.parser = spacy.load(parser)\n",
    "        self.dimension = -1\n",
    "\n",
    "    def load_wordvec(self, fname: str, nmax: int):\n",
    "        \"\"\"Load the Word2Vec weights of the given file into class variables. Maps each words to an id.\"\"\"\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as file:\n",
    "            next(file)\n",
    "            for i, line in enumerate(file):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        self.dimension = self.word2vec[word].shape[0]\n",
    "        print('Loaded %s pretrained word vectors of dimension %s' % (len(self.word2vec),  self.dimension))\n",
    "\n",
    "    @staticmethod\n",
    "    def sentence_treated(sentence: str) -> str:\n",
    "        \"\"\"Change the sentence construction to be more easily manageable and understandable.\"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.replace(\"'s\", ' is')\n",
    "        sentence = sentence.replace(\"n't\", ' not')\n",
    "        sentence = sentence.replace(\"'re\", ' are')\n",
    "        sentence = sentence.replace(\"'m\", ' am')\n",
    "        sentence = sentence.replace(\"'ve\", ' have')\n",
    "        sentence = sentence.replace(\"'ll\", ' will')\n",
    "        sentence = sentence.replace(\"'d\", ' would')\n",
    "        sentence = sentence.replace(\"-\", ' ')\n",
    "        sentence = sentence.replace(\"!\", ' ! ')\n",
    "        sentence = sentence.replace(\".\", ' . ')\n",
    "        sentence = sentence.replace(\":\", ' : ')\n",
    "        return sentence\n",
    "\n",
    "    def encode_parse(self, sentences: list) -> np.array:\n",
    "        \"\"\"\n",
    "        Takes a list of sentences, outputs a numpy array of sentence embeddings by computing the mean of words vector.\n",
    "        Also use a parser to keep only important words (adjectives, verbs, common or proper nouns and interjections.\n",
    "        If a word is unknown, ignore it, if a sentence is completely unknown, attribute a random vector as\n",
    "        representation.\n",
    "        \"\"\"\n",
    "        sentences_embedded = []\n",
    "        for sent in sentences:\n",
    "            sent = self.sentence_treated(sent)\n",
    "            words_weights = []\n",
    "            words_embedded = []\n",
    "            for word in self.parser(sent):\n",
    "                # Only keep important words, discard others\n",
    "                if word.pos_ in ['ADJ', 'VERB', 'NOUN', 'PROPN', 'INTJ']:\n",
    "                    str_word = str(word)\n",
    "                    # Get embedding vector of each word of the sentence\n",
    "                    try:\n",
    "                        words_embedded.append(self.word2vec[str_word])\n",
    "                        words_weights.append(1)\n",
    "                    except KeyError:\n",
    "                        # If word is unknown, ignore it.\n",
    "                        if len(words_embedded) == len(words_weights) + 1:  # 2 different lists are used\n",
    "                            words_weights.append(0)\n",
    "                        continue\n",
    "            # Average\n",
    "            if len(words_embedded) > 0:\n",
    "                sentences_embedded.append(np.average(words_embedded, weights=words_weights, axis=0))\n",
    "            else:\n",
    "                sentences_embedded.append(0.2 * np.random.rand(300) - 0.1)\n",
    "        return np.array(sentences_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150000 pretrained word vectors of dimension 300\n"
     ]
    }
   ],
   "source": [
    "w2v = PreTrainedWord2Vec(PATH_TO_WEIGHTS, 150000)\n",
    "clf = LogisticRegression(C=1, solver='liblinear', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_matrices(utterances, answers, is_training, nb_neg_examples=3):\n",
    "    if is_training:\n",
    "        if os.path.exists('data/X_tr.npy') and os.path.exists('data/Y_tr.npy'):\n",
    "            return np.load('data/X_tr.npy'), np.load('data/Y_tr.npy')\n",
    "    else:\n",
    "        if os.path.exists('data/X_val.npy') and os.path.exists('data/Y_val.npy'):\n",
    "            return np.load('data/X_val.npy'), np.load('data/Y_val.npy')\n",
    "    \n",
    "    X = np.array([])\n",
    "    Y = np.array([])\n",
    "    for idx_dialogue in range(len(utterances)):\n",
    "        for idx_utter in range(len(utterances[idx_dialogue])):\n",
    "            x_utter = w2v.encode_parse([utterances[idx_dialogue][idx_utter]])\n",
    "            x_answer = w2v.encode_parse(answers[idx_dialogue][idx_utter][:nb_neg_examples + 1])\n",
    "            y = np.zeros(len(x_answer))\n",
    "            y[0] = 1\n",
    "            if X.size:\n",
    "                X = np.vstack((X, np.hstack((x_utter * np.ones(x_answer.shape), x_answer))))\n",
    "                Y = np.hstack((Y, y))\n",
    "            else:\n",
    "                X = np.hstack((x_utter * np.ones(x_answer.shape), x_answer))\n",
    "                Y = y\n",
    "    \n",
    "    if is_training:\n",
    "        np.save('data/X_tr.npy', X)\n",
    "        np.save('data/Y_tr.npy', Y)\n",
    "    else:\n",
    "        np.save('data/X_val.npy', X)\n",
    "        np.save('data/Y_val.npy', Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr = get_training_matrices(utterances[:1000], answers[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29576, 600) (29576,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, Y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026939655172413795"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_tr, clf.predict(X_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
