{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advance Topic : Debiasing WordVectors\n",
    "###### Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
    "Link : https://arxiv.org/pdf/1607.06520.pdf\n",
    "\n",
    "Download the Glove Embeddings: https://www.kaggle.com/watts2/glove6b50dtxt  \n",
    "Download Our Trained Word2Vec SkipGram Model (from Google Drive) : https://drive.google.com/file/d/1VFW_F8YbwI0EsXfLkaTX2yuqfXVIxOIE/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_embedding(file,embedding_type):\n",
    "    \n",
    "    if embedding_type=='pretrained_glove':\n",
    "        with open(file, 'r') as f:\n",
    "            words = set()\n",
    "            word_to_vec_map = {}\n",
    "        \n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                curr_word = line[0]\n",
    "                words.add(curr_word)\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "                \n",
    "        return words, word_to_vec_map\n",
    "        \n",
    "                \n",
    "    elif embedding_type == 'our_trained_model':\n",
    "        \n",
    "        with open(file, 'rb') as w2v_dict:\n",
    "            dictio = pickle.load(w2v_dict)\n",
    "        return _, dictio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = 'data/glove.6B.50d.txt'\n",
    "pretrained_model_type = 'pretrained_glove'\n",
    "\n",
    "words, word_to_vec_dict = read_word_embedding(pretrained_model_path,'pretrained_glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Cosine_Similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Input:\n",
    "        u -- word vector      \n",
    "        v -- word vector\n",
    "\n",
    "    Output:\n",
    "        cosine_similarity -- the cosine similarity between u and v\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the dot product\n",
    "    dot_product = np.dot(u, v)\n",
    "    \n",
    "    # Compute the L2 norm of u\n",
    "    norm_u = np.sqrt(np.sum(u * u))\n",
    "    \n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v * v))\n",
    "    \n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Word Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(woman, lady) =  0.6054720738838377\n",
      "cosine_similarity(grandfather - grandmother, boy - girl) =  0.5053502916089266\n",
      "cosine_similarity(france - paris, india - delhi) =  0.6958505344885514\n"
     ]
    }
   ],
   "source": [
    "woman = word_to_vec_dict[\"woman\"]\n",
    "lady = word_to_vec_dict[\"lady\"]\n",
    "grandfather = word_to_vec_dict[\"grandfather\"]\n",
    "grandmother = word_to_vec_dict[\"grandmother\"]\n",
    "boy = word_to_vec_dict['boy']\n",
    "girl = word_to_vec_dict['girl']\n",
    "france = word_to_vec_dict[\"france\"]\n",
    "india = word_to_vec_dict[\"india\"]\n",
    "paris = word_to_vec_dict[\"paris\"]\n",
    "delhi = word_to_vec_dict[\"delhi\"]\n",
    "father = word_to_vec_dict[\"father\"]\n",
    "mother = word_to_vec_dict[\"mother\"]\n",
    "man = word_to_vec_dict['man']\n",
    "\n",
    "print(\"cosine_similarity(woman, lady) = \", cosine_similarity(woman, lady))\n",
    "print(\"cosine_similarity(grandfather - grandmother, boy - girl) = \", cosine_similarity(grandfather - grandmother, boy - girl))\n",
    "print(\"cosine_similarity(france - paris, india - delhi) = \",cosine_similarity(france - paris, india - delhi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was to check if our cosine similarity subroutine is working correctly. Now we move to the main task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogy And Understanding Gender Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Gender Bias Direction\n",
    "\n",
    "In order to compute Gender Bias direction $g$, we first compute $g_1=e_{woman} - e_{man}$, $g_2=e_{mother - father}$ and $g_3 = e_{girl} - e_{boy}$ and then take the average of them to get gender bias direction $g$. The paper referred uses more complicated method in involving Single Value Decomposition to find gender bias direction however our method is good for naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = woman - man\n",
    "g2 = mother - father\n",
    "g3 = girl - boy\n",
    "g = np.mean([g1, g2, g3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gendered Words Analysis\n",
    "\n",
    "Here we find the cosine similarity between the gender bias direction and the gendered words. Our results show that the cosine similarity between gendered words refering to women have high cosine similarity with our gender bias direction $g$ and cosine similarity between gendered words refering to men have negative cosine similarity with our gender bias direction $g$. This observation is expected as our gender bias vector is roughly in the direction of woman -> man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between gender bias vector and Gendered Words\n",
      "\n",
      "she 0.26306111282837885\n",
      "he -0.14241941924774518\n",
      "grandmother 0.381844691038982\n",
      "grandfather -0.07945446239749386\n",
      "jules -0.22475197624425813\n",
      "julia 0.2812390673350289\n",
      "paul -0.26727266762126545\n",
      "paula 0.24115895118999886\n",
      "female 0.3176843745802223\n",
      "male 0.31987771049104236\n",
      "sir -0.3350582031171057\n",
      "madame 0.1884288629212595\n"
     ]
    }
   ],
   "source": [
    "print ('Similarity between gender bias vector and Gendered Words\\n')\n",
    "\n",
    "# List of Gendered Words\n",
    "name_list = ['she','he','grandmother','grandfather','jules','julia','paul', 'paula', 'female','male','sir','madame']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_dict[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender-Neutral Word Analysis\n",
    "\n",
    "Here we calculate the cosine similarity between our gender bias direction $g$ and some gender neutral words. We observe that many words like nurse, receptionist have high similarity with gender bias vector whereas words like computer, politician, army have negative cosine similarity with gender bias direction $g$. Again this observation is expected as our gender bias vector is roughly in the direction of woman -> man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity between Gender Bias Direction and Gender Neutral words:\n",
      "\n",
      "president -0.21287766990605667\n",
      "scientist -0.14006801112016562\n",
      "babysitter 0.2719660457769091\n",
      "director -0.17198274531634614\n",
      "nurse 0.361028047426011\n",
      "science -0.058374259643848236\n",
      "arts 0.011760468125783751\n",
      "literature 0.02316946789375171\n",
      "warrior -0.16564638100307946\n",
      "doctor 0.0772141272665668\n",
      "pilot -0.03699357317847417\n",
      "receptionist 0.30167259871100655\n",
      "technology -0.16192108462558177\n",
      "fashion 0.1416547219136271\n",
      "teacher 0.10545901736578715\n",
      "engineer -0.22639944157426758\n",
      "pilot -0.03699357317847417\n",
      "computer -0.1682103192173514\n",
      "singer 0.20093000793226243\n",
      "army -0.24829029802501548\n",
      "politician -0.12186722698006967\n",
      "professor -0.08539369640933361\n"
     ]
    }
   ],
   "source": [
    "print('\\n Similarity between Gender Bias Direction and Gender Neutral words:\\n')\n",
    "word_list = ['president','scientist','babysitter','director', 'nurse', 'science', 'arts', 'literature', 'warrior','doctor', 'pilot', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer','army','politician','professor']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_dict[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "These results illustrate that the word embeddings have been induced with gender biases mainly due to the corpus they were trained on. The paper (refered in the begining) categorizes gender biases into two forms:\n",
    "* Direct Biases (refer to pdf attached for details)\n",
    "* Indirect Biases (refer to pdf attached for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiasing Word Vectors\n",
    "\n",
    "The paper describes that gender biased in word embeddings can be mitigated in two steps\n",
    "* Neutralization of Gender Neutral Words\n",
    "* Equalization of Gender Word Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Neutralization of Gender Neutral Words\n",
    "\n",
    "Given that we have an embedding $e$ for a gender neutral word like receptionist. The neutralization step removes the gender bias of gender neutral word by projecting it on the space, which is orthogonal to the gender bias axis.\n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}$$\n",
    "\n",
    "Here $e^{bias\\_component}$ as the projection of $e$ onto the direction $g$ and then we subtract this term from $e$ to get $e^{debiased}$. This is equivalent to orthogonal projection with respect to $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_embedding(word, g, word_to_vec_embedding):\n",
    "    \"\"\"\n",
    "    This function reduces the gender bias of gender-neutral word by projecting it\n",
    "    on the space orthogonal to the bias axis.\n",
    "    \n",
    "    Input:\n",
    "        word - input word we want to debias\n",
    "        g - gender bias direction\n",
    "        word_to_vec_dict - dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Output:\n",
    "        e_debiased - neutralized word vector representation of the input word embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get word embedding\n",
    "    e = word_to_vec_embedding[word]\n",
    "    \n",
    "    # Compute e_biased component\n",
    "    e_biascomponent = np.divide(np.dot(e,g),np.linalg.norm(g)**2) * g\n",
    " \n",
    "    # Neutralize e by substracting e_biascomponent from it. e_debiased is equal to its orthogonal projection\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between softball and gender_bias_direction, before neutralizing:  0.022254413015605105\n",
      "Cosine similarity between softball and gender_bias_direction, after neutralizing:  -5.19267285069332e-18\n",
      "\n",
      "\n",
      "Cosine similarity between receptionist and gender_bias_direction, before neutralizing:  0.30167259871100655\n",
      "Cosine similarity between receptionist and gender_bias_direction, after neutralizing:  1.4261260487443872e-17\n",
      "\n",
      "\n",
      "Cosine similarity between babysitter and gender_bias_direction, before neutralizing:  0.2719660457769091\n",
      "Cosine similarity between babysitter and gender_bias_direction, after neutralizing:  0.0\n",
      "\n",
      "\n",
      "Cosine similarity between homemaker and gender_bias_direction, before neutralizing:  0.36747726202795\n",
      "Cosine similarity between homemaker and gender_bias_direction, after neutralizing:  1.750335531198033e-17\n",
      "\n",
      "\n",
      "Cosine similarity between nurse and gender_bias_direction, before neutralizing:  0.361028047426011\n",
      "Cosine similarity between nurse and gender_bias_direction, after neutralizing:  2.2886443835013417e-17\n",
      "\n",
      "\n",
      "Cosine similarity between professor and gender_bias_direction, before neutralizing:  -0.08539369640933361\n",
      "Cosine similarity between professor and gender_bias_direction, after neutralizing:  -2.685594356916559e-17\n",
      "\n",
      "\n",
      "Cosine similarity between scientist and gender_bias_direction, before neutralizing:  -0.14006801112016562\n",
      "Cosine similarity between scientist and gender_bias_direction, after neutralizing:  3.073377586528001e-17\n",
      "\n",
      "\n",
      "Cosine similarity between football and gender_bias_direction, before neutralizing:  -0.2784594116374378\n",
      "Cosine similarity between football and gender_bias_direction, after neutralizing:  1.7970364168611256e-17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_neutral_words = [\"softball\",\"receptionist\", 'babysitter', 'homemaker','nurse','professor','scientist','football']\n",
    "\n",
    "for w in gender_neutral_words:\n",
    "    print(\"Cosine similarity between \" + w + \" and gender_bias_direction, before neutralizing: \", cosine_similarity(word_to_vec_dict[w], g))\n",
    "    e_debiased = neutralize_embedding(w, g, word_to_vec_dict)\n",
    "    print(\"Cosine similarity between \" + w + \" and gender_bias_direction, after neutralizing: \", cosine_similarity(e_debiased, g))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* We observe that after neutralization step the cosine similarity between gender neutral words and gender bias direction has shrinked to almost zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Equalize Gender Word Pairs\n",
    "\n",
    "\n",
    "By applying neutralizing to \"computer\" we can reduce the gender-stereotype related with it. But this does not guarantee that word pair (\"he\",\"she\") are equidistant from \"computer\". Therefore apart from neutralizing gender neutral words, we also need to apply Equalization to word pairs like (grandfather,grandmother) and (actor,actress) to ensure that such word-pairs differ only in the gender property.\n",
    "\n",
    "\n",
    "The main idea behind equalization is to make sure that such particular pair of words are equi-distant from $g_\\perp$, where $g_\\perp$ represent a vector perpendicular to gender bias direction $g$. This step also ensures that the equalized word pair eg. (\"male\",\"female\") are now the same distance from debiased gender neutral word like $e_{computer}^{debiased}$, $e_{professor}^{debiased}$, $e_{scientist}^{debiased}$.\n",
    "\n",
    "For details refer to the pdf attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair,g, word_to_vec_dict):\n",
    "    \"\"\"\n",
    "    Debias gender specific words using Equalization Method\n",
    "    \n",
    "    Input:\n",
    "    pair - Gender specific words to debias\n",
    "    g - gender bias direction/axis\n",
    "    word_to_vec_dict - dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Output\n",
    "    e1_equalized -- word vector corresponding to the first word\n",
    "    e2_equalized -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    #Get Word Vector Embeddings to be Neutralized\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_dict[w1], word_to_vec_dict[w2]\n",
    "    \n",
    "    #Compute the mean of two word vectors\n",
    "    mu = (e_w1 + e_w2)/2.0\n",
    "\n",
    "    # Compute the projections of mean (computed above) over the gender bias axis and the orthogonal axis\n",
    "    mu_B = np.divide(np.dot(mu, g),np.linalg.norm(g)**2)*g\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Compute e_w1B and e_w2B\n",
    "    e_w1B = np.divide(np.dot(e_w1, g),np.linalg.norm(g)**2)*g\n",
    "    e_w2B = np.divide(np.dot(e_w2, g),np.linalg.norm(g)**2)*g\n",
    "        \n",
    "    # Adjusting the gender bias part of e_w1B and e_w2B\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1-np.sum(mu_orth**2)))*np.divide(e_w1B-mu_B, np.abs(e_w1-mu_orth-mu_B))\n",
    "    corrected_e_w2B = np.sqrt(np.abs(1-np.sum(mu_orth**2)))*np.divide(e_w2B-mu_B, np.abs(e_w2-mu_orth-mu_B))\n",
    "\n",
    "    # Debias by equalizing e1 and e2 to the sum of their corrected projections\n",
    "    e1_equalized = corrected_e_w1B + mu_orth\n",
    "    e2_equalized = corrected_e_w2B + mu_orth\n",
    "                                                                \n",
    "    return e1_equalized, e2_equalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities before equalizing:\n",
      "masculine ,gender_bias_direction =  0.20366255668907882\n",
      "feminine ,gender_bias_direction =  0.3068077556241729\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "masculine _equalized, gender_bias_direction =  -0.5534152510908446\n",
      "feminine _equalized, gender_bias_direction =  0.5505405761605079\n",
      "---------------------------\n",
      "\n",
      "Cosine similarities before equalizing:\n",
      "actor ,gender_bias_direction =  -0.048377516575346544\n",
      "actress ,gender_bias_direction =  0.4053361624508814\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "actor _equalized, gender_bias_direction =  -0.3644064148498128\n",
      "actress _equalized, gender_bias_direction =  0.3688567757977562\n",
      "---------------------------\n",
      "\n",
      "Cosine similarities before equalizing:\n",
      "man ,gender_bias_direction =  -0.02435875412347576\n",
      "woman ,gender_bias_direction =  0.3979047171251496\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "man _equalized, gender_bias_direction =  -0.40760482687217325\n",
      "woman _equalized, gender_bias_direction =  0.40550412629391397\n",
      "---------------------------\n",
      "\n",
      "Cosine similarities before equalizing:\n",
      "grandfather ,gender_bias_direction =  -0.07945446239749386\n",
      "grandmother ,gender_bias_direction =  0.381844691038982\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "grandfather _equalized, gender_bias_direction =  -0.16576685673507144\n",
      "grandmother _equalized, gender_bias_direction =  0.1658115856861888\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gendered_words = [('masculine','feminine'),('actor','actress'),('man','woman'),('grandfather','grandmother')]\n",
    "\n",
    "for pair in gendered_words:\n",
    "    \n",
    "    w1,w2 = pair\n",
    "        \n",
    "    print(\"Cosine similarities before equalizing:\")\n",
    "    print( w1 ,\",gender_bias_direction = \", cosine_similarity(word_to_vec_dict[w1], g))\n",
    "    print( w2 ,\",gender_bias_direction = \", cosine_similarity(word_to_vec_dict[w2], g))\n",
    "    print('\\n')\n",
    "    \n",
    "    w1_equalized, w2_equalized = equalize((w1, w2), g, word_to_vec_dict)\n",
    "    \n",
    "    print(\"Cosine similarities after equalizing:\")\n",
    "    print(w1,\"_equalized, gender_bias_direction = \",cosine_similarity(w1_equalized, g))\n",
    "    print(w2,\"_equalized, gender_bias_direction = \",cosine_similarity(w2_equalized, g))\n",
    "    print('---------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "* We observe that after Equalization step the cosine similarity between gender word pairs and gender bias direction is nearly the same. The negative sign is due to the opposite direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Gender Bias in Our Trained Model\n",
    "Download Our Trained Word2Vec SkipGram Model (from Google Drive) : https://drive.google.com/file/d/1VFW_F8YbwI0EsXfLkaTX2yuqfXVIxOIE/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_trained_model_path = 'data/model.pkl'\n",
    "our_model_type = 'our_trained_model'\n",
    "\n",
    "_ , word_to_vec_dict_custom = read_word_embedding(our_trained_model_path,'our_trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Analogy of Our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(king-man,queen-woman) =  0.9894141595481635\n",
      "cosine_similarity(he - she, father - mother) =  0.7840329622083684\n",
      "cosine_similarity(france - paris, india - delhi) =  -0.6901133171836422\n"
     ]
    }
   ],
   "source": [
    "woman = word_to_vec_dict_custom[\"woman\"]\n",
    "lady = word_to_vec_dict_custom[\"lady\"]\n",
    "\n",
    "he = word_to_vec_dict_custom['he']\n",
    "she = word_to_vec_dict_custom['she']\n",
    "\n",
    "father = word_to_vec_dict_custom[\"father\"]\n",
    "mother = word_to_vec_dict_custom[\"mother\"]\n",
    "man = word_to_vec_dict_custom['man']\n",
    "\n",
    "king = word_to_vec_dict_custom['king']\n",
    "queen = word_to_vec_dict_custom['queen']\n",
    "\n",
    "france = word_to_vec_dict_custom[\"france\"]\n",
    "india = word_to_vec_dict_custom[\"india\"]\n",
    "paris = word_to_vec_dict_custom[\"paris\"]\n",
    "delhi = word_to_vec_dict_custom[\"delhi\"]\n",
    "\n",
    "print(\"cosine_similarity(king-man,queen-woman) = \", cosine_similarity(king-man, queen-woman))\n",
    "print(\"cosine_similarity(he - she, father - mother) = \",cosine_similarity(he - she, father - mother))\n",
    "print(\"cosine_similarity(france - paris, india - delhi) = \",cosine_similarity(france - paris, india - delhi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "We observe that $king - man$ has high cosine similarity to $queen - woman$. This is a good indication for our model but we also observe that $france-paris$ has very less similarity to $india-delhi$, which highlights that the current checkout of our model is not able characetristics of such word vectors. We believe using a bigger training dataset will help to mitigate the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Gender Bias Direction\n",
    "\n",
    "Here we are only using vectors which are present in our vocabulary to avoid keyvalue error. But we illustrate the same ideas for debiasing we used for pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = woman - man\n",
    "g2 = mother - father\n",
    "\n",
    "# Gender Bias Direction\n",
    "g_custom = np.mean([g1, g2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "In order to calculate the gender bias direction, we calculate $g_1 = e_{woman} - e_{man}$ and $g_2 = e_{mother} - e_{father}$. We then calculate the average of $g_1$ and $g_2$ to get $g_{custom}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gendered Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between gender bias vector and Gendered Words\n",
      "\n",
      "queen 0.9539632301982861\n",
      "he -0.9347621670824781\n",
      "female 0.5161396730720444\n",
      "son -0.9629745472263926\n"
     ]
    }
   ],
   "source": [
    "print ('Similarity between gender bias vector and Gendered Words\\n')\n",
    "\n",
    "# List of Gendered Words\n",
    "name_list = ['queen','he', 'female','son']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_dict_custom[w], g_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment  \n",
    "Our results show that the cosine similarity between gendered words refering to women (like women and female) have high cosine similarity with our gender bias direction $g$ and cosine similarity between gendered words refering to men (like he and king) have negative cosine similarity with our gender bias direction $g$. This observation is expected as our gender bias vector is roughly in the direction of woman -> man."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Neutral Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity between Gender Bias Direction and Gender Neutral words:\n",
      "\n",
      "president -0.9469821780988354\n",
      "director 0.9652766500364007\n",
      "science 0.4161949662159972\n",
      "arts -0.9302221216207309\n",
      "pilot 0.9697341256697147\n",
      "computer -0.9647791208778314\n",
      "singer -0.9308793410597123\n",
      "army 0.9697341256697147\n",
      "professor -0.22536222173972303\n"
     ]
    }
   ],
   "source": [
    "print('\\n Similarity between Gender Bias Direction and Gender Neutral words:\\n')\n",
    "word_list = ['president','director', 'science', 'arts', 'pilot', 'computer', 'singer','army','professor']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_dict_custom[w], g_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "Here we calculate the cosine similarity between our gender bias direction $g_{custom}$ and some gender neutral words. We observe that many words like president, computer have high dissimilarity with gender bias vector. We also observe that many of results are not accurate like computer has high similarity with gender bias direction which is unlike the observation we had with Glove. Again this observation is expected as our gender bias vector is roughly in the direction of woman -> man."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Neutralization of Gender Neutral Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between director and gender_bias_direction, before neutralizing:  0.9652766500364007\n",
      "Cosine similarity between director and gender_bias_direction, after neutralizing:  -4.3002141702867013e-16\n",
      "\n",
      "\n",
      "Cosine similarity between singer and gender_bias_direction, before neutralizing:  -0.9308793410597123\n",
      "Cosine similarity between singer and gender_bias_direction, after neutralizing:  4.970600259101881e-16\n",
      "\n",
      "\n",
      "Cosine similarity between captain and gender_bias_direction, before neutralizing:  0.9697341256697147\n",
      "Cosine similarity between captain and gender_bias_direction, after neutralizing:  -1.2584502521366135e-15\n",
      "\n",
      "\n",
      "Cosine similarity between computer and gender_bias_direction, before neutralizing:  -0.9647791208778314\n",
      "Cosine similarity between computer and gender_bias_direction, after neutralizing:  6.5526270582148e-16\n",
      "\n",
      "\n",
      "Cosine similarity between president and gender_bias_direction, before neutralizing:  -0.9469821780988354\n",
      "Cosine similarity between president and gender_bias_direction, after neutralizing:  6.877594335823285e-16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_neutral_words = ['director','singer','captain','computer','president']\n",
    "\n",
    "for w in gender_neutral_words:\n",
    "    print(\"Cosine similarity between \" + w + \" and gender_bias_direction, before neutralizing: \", cosine_similarity(word_to_vec_dict_custom[w], g_custom))\n",
    "    e_debiased = neutralize_embedding(w, g_custom, word_to_vec_dict_custom)\n",
    "    print(\"Cosine similarity between \" + w + \" and gender_bias_direction, after neutralizing: \", cosine_similarity(e_debiased, g_custom))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "* We observe that after neutralization step the cosine similarity between gender neutral words and gender bias direction $g_{custom}$ has shrinked to almost zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize Gender Word Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities before equalizing:\n",
      "male ,gender_bias_direction =  0.8011600149806818\n",
      "female ,gender_bias_direction =  0.5161396730720444\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "male _equalized, gender_bias_direction =  0.42334598229937503\n",
      "female _equalized, gender_bias_direction =  -0.42357656917384223\n",
      "---------------------------\n",
      "\n",
      "Cosine similarities before equalizing:\n",
      "he ,gender_bias_direction =  -0.9347621670824781\n",
      "she ,gender_bias_direction =  -0.9744627890989416\n",
      "\n",
      "\n",
      "Cosine similarities after equalizing:\n",
      "he _equalized, gender_bias_direction =  0.5245384902363578\n",
      "she _equalized, gender_bias_direction =  -0.5244428241151656\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gendered_words = [('male','female'),('he','she')]\n",
    "\n",
    "for pair in gendered_words:\n",
    "    \n",
    "    w1,w2 = pair\n",
    "        \n",
    "    print(\"Cosine similarities before equalizing:\")\n",
    "    print( w1 ,\",gender_bias_direction = \", cosine_similarity(word_to_vec_dict_custom[w1], g_custom))\n",
    "    print( w2 ,\",gender_bias_direction = \", cosine_similarity(word_to_vec_dict_custom[w2], g_custom))\n",
    "    print('\\n')\n",
    "    \n",
    "    w1_equalized, w2_equalized = equalize((w1, w2), g_custom, word_to_vec_dict_custom)\n",
    "    \n",
    "    print(\"Cosine similarities after equalizing:\")\n",
    "    print(w1,\"_equalized, gender_bias_direction = \",cosine_similarity(w1_equalized, g_custom))\n",
    "    print(w2,\"_equalized, gender_bias_direction = \",cosine_similarity(w2_equalized, g_custom))\n",
    "    print('---------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "* We observe that after Equalization step the cosine similarity between gender word pairs and gender bias direction is nearly the same. The negative sign is due to the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
